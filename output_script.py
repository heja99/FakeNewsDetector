

# Code cells extracted from Jupyter notebook


# In[ ]:
# Importing necessary libraries
import numpy as np  # For numerical operations
import pandas as pd  # For data manipulation and analysis
import matplotlib.pyplot as plt  # For plotting graphs
import seaborn as sns  # For data visualization
from wordcloud import WordCloud, STOPWORDS  # For creating word clouds
import nltk  # Natural Language Toolkit
from nltk.corpus import stopwords  # Stopwords for text processing
import re  # Regular expressions for text processing
import gensim  # For topic modeling and NLP
from gensim.utils import simple_preprocess  # Preprocessing for Gensim
from gensim.parsing.preprocessing import STOPWORDS  # Stopwords for Gensim
import nbformat # Reading and writing in notebooks
import plotly.express as px  # For interactive plots
from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets
from sklearn.feature_extraction.text import CountVectorizer  # For converting text data to numerical format
from sklearn.linear_model import LogisticRegression  # Logistic Regression model
from sklearn.metrics import roc_auc_score  # ROC AUC score for model evaluation
from sklearn.metrics import confusion_matrix  # Confusion matrix for model evaluation
import tkinter as tk # Pop-Up window
from tkinter import Text, Label, Button, Scrollbar # UI elements pop-up window

# Downloading necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')


# In[ ]:
# Use read_csv to read the two data sets
df_true = pd.read_csv("./dataset/True.csv")
df_fake = pd.read_csv("./dataset/Fake.csv")

# In[ ]:
# Assign a 'target' column with value 1 to df_true indicating it is a true sample.
df_true['target'] = 1
# Assign a 'target' column with value 0 to df_fake indicating it is a fake sample.
df_fake['target'] = 0

# Concatenate df_true and df_fake DataFrames vertically, reset the index, and store the result in the variable df.
df = pd.concat([df_true, df_fake]).reset_index(drop=True)

# Create a new column 'original' in df by combining the 'title' and 'text' columns with a space in between.
df['original'] = df['title'] + ' ' + df['text']

# Display the first few rows of the resulting DataFrame df.
df.head()

# In[ ]:
df.isnull().sum()

# In[ ]:
# Importing the stopwords module from NLTK (Natural Language Toolkit)
stop_words = stopwords.words('english')

# Adding custom stopwords to the list
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])

# Defining a function for text preprocessing
def preprocess(text):
    result = []  # Initialize an empty list to store preprocessed tokens
    
    # Iterate through tokens generated by Gensim's simple_preprocess
    for token in gensim.utils.simple_preprocess(text):
        # Check if the token is not in Gensim's predefined stopwords,
        # has a length greater than 2, and is not in the custom stop_words list
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2 and token not in stop_words:
            result.append(token)  # Append the token to the result list
    
    return result  # Return the list of preprocessed tokens

# In[ ]:
# Transforming the unmatching subjects to the same notation
df.subject=df.subject.replace({'politics':'PoliticsNews','politicsNews':'PoliticsNews'})

# In[ ]:
# Grouping the DataFrame 'df' by the 'target' column and counting the occurrences of 'title' for each group
sub_tf_df = df.groupby('target').apply(lambda x: x['title'].count()).reset_index(name='Counts')

# Replacing numerical values in the 'target' column with corresponding labels 'False' and 'True'
sub_tf_df.target.replace({0: 'False', 1: 'True'}, inplace=True)

# Creating a bar plot using Plotly Express with 'target' on the x-axis, 'Counts' on the y-axis, and colored by 'Counts'
fig = px.bar(sub_tf_df, x="target", y="Counts",
             color='Counts', barmode='group',
             height=400)

# Displaying the plot
fig.show()

# In[ ]:
sub_check=df.groupby('subject').apply(lambda x:x['title'].count()).reset_index(name='Counts')
fig=px.bar(sub_check,x='subject',y='Counts',color='Counts',title='Count of News Articles by Subject')
fig.show()

# In[ ]:
# Applying the 'preprocess' function to the 'title' column of the DataFrame 'df' to clean and tokenize the text
df['clean_title'] = df['title'].apply(preprocess)

# Displaying the cleaned and tokenized title for the first rows in the 'clean_title' column
print(df['clean_title'].head())

# Creating a new column 'clean_joined_title' by joining the cleaned and tokenized words into a single string for each row
df['clean_joined_title'] = df['clean_title'].apply(lambda x: " ".join(x))

# In[ ]:
# Setting up the figure size for the word cloud visualization
plt.figure(figsize=(20, 20))

# Generating a word cloud for rows in the DataFrame where 'target' is equal to 1
wc_positive = WordCloud(max_words=2000, width=1600, height=800, stopwords=stop_words).generate(" ".join(df[df.target == 1].clean_joined_title))

# Hiding the x and y axes
plt.axis('off')

# Removing ticks on the axes
plt.xticks([]), plt.yticks([])

# Displaying the word cloud using imshow with bilinear interpolation
plt.imshow(wc_positive, interpolation='bilinear')
plt.show()

# In[ ]:
# Setting up the figure size for the word cloud visualization
plt.figure(figsize=(20, 20))

# Generating a word cloud for rows in the DataFrame where 'target' is equal to 1
wc_positive = WordCloud(max_words=2000, width=1600, height=800, stopwords=stop_words).generate(" ".join(df[df.target == 0].clean_joined_title))

# Hiding the x and y axes
plt.axis('off')

# Removing ticks on the axes
plt.xticks([]), plt.yticks([])

# Displaying the word cloud using imshow with bilinear interpolation
plt.imshow(wc_positive, interpolation='bilinear')
plt.show()

# In[ ]:
# Initializing a variable to store the maximum number of words in a title
maxlen = -1

# Iterating through each document in the 'clean_joined_title' column of the DataFrame 'df'
for doc in df.clean_joined_title:
    # Tokenizing the document into words using NLTK's word_tokenize function
    tokens = nltk.word_tokenize(doc)
    
    # Updating maxlen if the number of tokens in the current document is greater
    if maxlen < len(tokens):
        maxlen = len(tokens)

# Displaying the maximum number of words found in any title
print("Maximum number of words in a title: ", maxlen)

# Creating a histogram using Plotly Express to visualize the distribution of word counts in titles
fig = px.histogram(x=[len(nltk.word_tokenize(x)) for x in df.clean_joined_title], nbins=50)

# Displaying the histogram
fig.show()

# In[ ]:
# Train/ test split with in a 80/ 20 ratio
X_train, X_test, y_train, y_test = train_test_split(df.clean_joined_title, df.target, test_size = 0.2,random_state=2)
vec_train_1 = CountVectorizer().fit(X_train)
X_vec_train = vec_train_1.transform(X_train)
X_vec_test = vec_train_1.transform(X_test)

# In[ ]:
# Use LogisticRegression() as predictive modelling algorithm and fit model
model_1 = LogisticRegression(C=2, max_iter=1000)
model_1.fit(X_vec_train, y_train)

# Do the prediction
predicted_value = model_1.predict(X_vec_test)

# Get and print accuracy via "area under the curve" score
accuracy_value = roc_auc_score(y_test, predicted_value)
print(f"Accuracy Logistic Regression: {round(accuracy_value, 4)}")

# In[ ]:
# Create and plot confusion matrix to compare predicted with actual value
cm = confusion_matrix(list(y_test), predicted_value)
plt.figure(figsize = (10, 10))
# Add heatmap according to the values within the confusion matrix
sns.heatmap(cm, annot = True,fmt='g')

# In[ ]:
# Apply the 'preprocess' function to each element in the 'text' column of the DataFrame 'df'
df['clean_text'] = df['text'].apply(preprocess)

# Apply a lambda function to join the preprocessed tokens into a single string for each row
df['clean_joined_text'] = df['clean_text'].apply(lambda x: " ".join(x))

# In[ ]:
# Set the figure size
plt.figure(figsize=(20, 20))

# Generate a WordCloud for the 'clean_joined_text' column of rows where 'target' is 1
wc = WordCloud(max_words=2000, width=1600, height=800, stopwords=stop_words).generate(" ".join(df[df.target == 1].clean_joined_text))

# Display the WordCloud image using imshow
plt.imshow(wc, interpolation='bilinear')

# Remove x and y axes
plt.axis('off')

# Show the plot
plt.show()

# In[ ]:
# Set the figure size
plt.figure(figsize=(20, 20))

# Generate a WordCloud for the 'clean_joined_text' column of rows where 'target' is 1
wc = WordCloud(max_words=2000, width=1600, height=800, stopwords=stop_words).generate(" ".join(df[df.target == 1].clean_joined_text))

# Display the WordCloud image using imshow
plt.imshow(wc, interpolation='bilinear')

# Remove x and y axes
plt.axis('off')

# Show the plot
plt.show()

# In[ ]:
# Initialize a variable to track the maximum number of words
maxlen = -1

# Iterate over each document in the 'clean_joined_text' column of the DataFrame 'df'
for doc in df.clean_joined_text:
    # Tokenize the document into words using nltk
    tokens = nltk.word_tokenize(doc)
    
    # Check if the current document has more words than the current maximum
    if maxlen < len(tokens):
        maxlen = len(tokens)

# Print the maximum number of words in a news content
print("Maximum number of words in the text of a news article:", maxlen)

# Create a histogram using Plotly Express to visualize the distribution of the number of words in each document
fig = px.histogram(x=[len(nltk.word_tokenize(x)) for x in df.clean_joined_text], nbins=50)

# Show the histogram
fig.show()

# In[ ]:
# Train/ test split with in a 80/ 20 ratio
X_train, X_test, y_train, y_test = train_test_split(df.clean_joined_text, df.target, test_size = 0.2,random_state=2)
vec_train = CountVectorizer().fit(X_train)
X_vec_train = vec_train.transform(X_train)
X_vec_test = vec_train.transform(X_test)

# In[ ]:
# Use LogisticRegression() as predictive modelling algorithm and fit model
model = LogisticRegression(C=2.5, max_iter=1000)
model.fit(X_vec_train, y_train)

# Do prediction
predicted_value = model.predict(X_vec_test)

# Get and print accuracy via "area under the curve" score
accuracy_value = roc_auc_score(y_test, predicted_value)
print(f"Accuracy Logistic Regression: {round(accuracy_value, 4)}")

# In[ ]:
# Create and plot confusion matrix to compare predicted values with actual values
prediction = []
for i in range(len(predicted_value)):
    if predicted_value[i].item() > 0.5:
        prediction.append(1)
    else:
        prediction.append(0)
cm = confusion_matrix(list(y_test), prediction)
plt.figure(figsize = (10, 10))
# Add heatmap according to the values within the confusion matrix
sns.heatmap(cm, annot = True,fmt='g')

# In[ ]:
# Apply the 'preprocess' function to each element in the 'original' column of our data frame
df['clean_final'] = df['original'].apply(preprocess)

# Apply a lambda function to join the preprocessed tokens into a single string for each row
df['clean_joined_final'] = df['clean_final'].apply(lambda x: " ".join(x))

# In[ ]:
# Train/ test split with in a 80/ 20 ratio
X_train, X_test, y_train, y_test = train_test_split(df.clean_joined_final, df.target, test_size = 0.2,random_state=0)
vec_train = CountVectorizer().fit(X_train)
X_vec_train = vec_train.transform(X_train)
X_vec_test = vec_train.transform(X_test)

# In[ ]:
# Use LogisticRegression() as predictive modelling algorithm and fit model
model = LogisticRegression(C=3, max_iter=1000)
model.fit(X_vec_train, y_train)

# Do prediction
predicted_value = model.predict(X_vec_test)

# Get and print accuracy via "area under curve" score
accuracy_value = roc_auc_score(y_test, predicted_value)
print(f"Accuracy Logistic Regression: {round(accuracy_value, 4)}")

# In[ ]:
# Create and plot confusion matrix to compare predicted values with actual values
prediction = []
for i in range(len(predicted_value)):
    if predicted_value[i].item() > 0.5:
        prediction.append(1)
    else:
        prediction.append(0)
cm = confusion_matrix(list(y_test), prediction)
plt.figure(figsize = (10, 10))

# Add heatmap according to the values in the confusion matrix
sns.heatmap(cm, annot = True,fmt='g')

# In[ ]:
# Find the first five rows where the 'target' column is 1
not_fake_examples = df[df['target'] == 1].head(3)

# Print the titles of the examples
print("Five Examples of Non-Fake News Titles:")
for index, example in not_fake_examples.iterrows():
    print(f"{index + 1}. {example['title']}")

print()

# Find the first three rows where the 'target' column is 0
fake_examples = df[df['target'] == 0].head(3)

# Print the titles of the examples
print("Three Examples of Fake News Titles:")
for index, example in fake_examples.iterrows():
    print(f"{index + 1}. {example['title']}")


# In[ ]:
# Function to analyze text and display result
def analyze_text():
    input_text = entry.get("1.0", "end-1c")  # Get text from the entry widget
    cleaned_text = " ".join(preprocess(input_text))  # Preprocess the input text
    vectorized_text = vec_train_1.transform([cleaned_text])  # Vectorize the preprocessed text
    prediction = model_1.predict(vectorized_text)  # Make a prediction
    result_label.config(text=f"Prediction: {'Fake' if prediction[0] == 0 else 'True'}")  # Display the prediction

# Create the main Tkinter window
window = tk.Tk()
window.title("Fake News Detector")

# Create and place GUI components
label = Label(window, text="Enter Text:")
label.pack()

entry = Text(window, height=10, width=50, wrap="word")
entry.pack()

analyze_button = Button(window, text="Analyze", command=analyze_text)
analyze_button.pack()

result_label = Label(window, text="Prediction: ")
result_label.pack()

# Start the Tkinter event loop
window.mainloop()

# In[ ]:
from flask import Flask, render_template, request
import nltk
from nltk.corpus import stopwords
from gensim.utils import simple_preprocess
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from flask_ngrok import run_with_ngrok

app = Flask(__name__)
run_with_ngrok(app)

# Assuming 'preprocess' is a function defined for text preprocessing
def preprocess(text):
    stop_words = stopwords.words('english')
    result = [token for token in simple_preprocess(text) if token not in stop_words]
    return result

# Load your trained model and vectorizer
# (Assuming vec_train and model are already defined and trained in your code)
vec_train = CountVectorizer()
model = LogisticRegression()

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze():
    input_text = request.form['text']
    cleaned_text = " ".join(preprocess(input_text))
    vectorized_text = vec_train_1.transform([cleaned_text])
    prediction = model_1.predict(vectorized_text)
    result = 'Fake' if prediction[0] == 0 else 'True'
    return render_template('index.html', result=result, input_text=input_text)

if __name__ == '__main__':
    app.run()